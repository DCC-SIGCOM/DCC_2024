{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yolov5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m  \u001b[38;5;66;03m# OpenCV 사용\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01myolov5\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YOLOv5  \u001b[38;5;66;03m# YOLOv5 라이브러리 사용 가정\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'yolov5'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import cv2  # OpenCV 사용\n",
    "import numpy as np\n",
    "from yolov5 import YOLOv5  # YOLOv5 라이브러리 사용 가정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 사용 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# YOLOv5 모델 불러오기 (YOLOv5 모델 경로 필요)\n",
    "# YOLOv5는 사전학습된 모델을 제공하며, 여기서는 \"yolov5s.pt\" 모델을 사용합니다.\n",
    "# 이 모델은 COCO 데이터셋으로 사전학습되어 사람, 차량 등 여러 객체를 감지할 수 있습니다.\n",
    "yolov5_model_path = \"yolov5s.pt\"  # YOLOv5 사전학습 모델 경로\n",
    "yolo = YOLOv5(yolov5_model_path, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset Class 정의\n",
    "class GenderStyleDataset(Dataset):\n",
    "    def __init__(self, image_folder, label_mapping, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.image_files = [f for f in os.listdir(image_folder) if f.endswith('.jpg')]\n",
    "        self.transform = transform\n",
    "        self.label_mapping = label_mapping\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_folder, img_name)\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        # 파일명에서 성별과 스타일 추출\n",
    "        parts = img_name.split('_')\n",
    "        style = parts[3]  # 예: sportivecasual\n",
    "        gender_code = parts[-1][0]  # M or W\n",
    "\n",
    "        if gender_code == 'M':\n",
    "            gender = 'man'\n",
    "        elif gender_code == 'W':\n",
    "            gender = 'woman'\n",
    "\n",
    "        # 성별 & 스타일을 결합한 클래스 라벨\n",
    "        class_label = f\"{gender}_{style}\"\n",
    "\n",
    "        # 이미지에서 옷 부분만 추출 (YOLO 사용)\n",
    "        image_np = np.array(image)  # PIL 이미지를 NumPy 배열로 변환\n",
    "        results = yolo.predict(image_np)  # YOLO를 사용해 객체 감지 수행\n",
    "\n",
    "        # 옷만 추출 (person 클래스가 있는 바운딩 박스를 사용한다고 가정)\n",
    "        clothes_images = []\n",
    "        for result in results:\n",
    "            if result['class'] == 'person':\n",
    "                x_min, y_min, x_max, y_max = result['box']\n",
    "                cropped_image = image_np[y_min:y_max, x_min:x_max]\n",
    "                clothes_images.append(cropped_image)\n",
    "        \n",
    "        # 여러 옷 이미지가 추출되었을 경우 첫 번째만 사용 (필요시 변경 가능)\n",
    "        if clothes_images:\n",
    "            image_cropped = Image.fromarray(clothes_images[0])\n",
    "        else:\n",
    "            image_cropped = image  # 옷이 감지되지 않으면 원본 사용\n",
    "\n",
    "        # Transform 적용\n",
    "        if self.transform:\n",
    "            image_cropped = self.transform(image_cropped)\n",
    "\n",
    "        # 문자열 라벨을 숫자 라벨로 변환\n",
    "        label = self.label_mapping[class_label]\n",
    "\n",
    "        return image_cropped, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 라벨 인덱스 매핑 생성\n",
    "def create_label_mapping(image_folder):\n",
    "    label_set = set()\n",
    "    for filename in os.listdir(image_folder):\n",
    "        parts = filename.split('_')\n",
    "        style = parts[3]\n",
    "        gender_code = parts[-1][0]\n",
    "        if gender_code == 'M':\n",
    "            gender = 'man'\n",
    "        elif gender_code == 'W':\n",
    "            gender = 'woman'\n",
    "        class_label = f\"{gender}_{style}\"\n",
    "        label_set.add(class_label)\n",
    "    label_mapping = {label: idx for idx, label in enumerate(sorted(label_set))}\n",
    "    return label_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 데이터 경로 및 변환 설정\n",
    "training_image_dir = '/home/gyuha_lee/DCC2024/dataset/training_image'\n",
    "validation_image_dir = '/home/gyuha_lee/DCC2024/dataset/validation_image'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 라벨 매핑 생성\n",
    "label_mapping = create_label_mapping(train_image_folder)\n",
    "num_classes = len(label_mapping)\n",
    "\n",
    "# 데이터셋 및 DataLoader 생성\n",
    "train_dataset = GenderStyleDataset(image_folder=train_image_folder, label_mapping=label_mapping, transform=transform)\n",
    "val_dataset = GenderStyleDataset(image_folder=val_image_folder, label_mapping=label_mapping, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ResNet-18 모델 정의 및 GPU로 전송\n",
    "model = models.resnet18(pretrained=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)  # 마지막 레이어 수정\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 손실 함수와 옵티마이저 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 학습 루프 및 검증 루프\n",
    "num_epochs = 50  # 에폭 수 설정\n",
    "for epoch in range(num_epochs):\n",
    "    # 에폭 시작 정보 출력\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} 시작\")\n",
    "\n",
    "    # 학습 단계\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # GPU로 전송\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # 에폭 학습 완료 후 손실 출력\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] 학습 완료, Average Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # 검증 단계\n",
    "    model.eval()  # 모델 평가 모드로 설정\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_inputs, val_labels = val_inputs.to(device), val_labels.to(device)  # GPU로 전송\n",
    "\n",
    "            val_outputs = model(val_inputs)\n",
    "            val_loss += criterion(val_outputs, val_labels).item()\n",
    "\n",
    "            _, predicted = torch.max(val_outputs, 1)\n",
    "            total += val_labels.size(0)\n",
    "            correct += (predicted == val_labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * correct / total\n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    # 다시 학습 모드로 전환\n",
    "    model.train()\n",
    "\n",
    "# 사전학습 모델을 사용하지 않는 경우, 특정 클래스(예: 옷, 사람 등)를 감지하도록 YOLO 모델을 직접 학습시켜야 합니다.\n",
    "# 이를 위해서는 해당 클래스에 대한 라벨이 포함된 데이터셋이 필요하며, 이를 사용해 YOLO 모델을 학습해야 합니다.\n",
    "# 학습된 YOLO 모델은 객체 감지 성능이 사전학습 모델보다 떨어질 수 있지만, 원하는 특정 객체에 대해 더 높은 정확도를 보일 수 있습니다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dcc2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
